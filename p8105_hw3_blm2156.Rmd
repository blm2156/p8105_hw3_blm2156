---
title: "p8105_hw3_blm2156"
author: Britney Mazzetta - blm2156
output: github_document
---
# Problem 1
```{r}
library(tidyverse)
library(p8105.datasets)
library(knitr)
data("instacart")
```

## Description of Dataset 
There are `r nrow(instacart)` rows in the instacart dataset. There are `r ncol(instacart)` columns in the instacart dataset. 

There are `r nrow(distinct(instacart, aisle))` distinct aisles in the dataset. 

## Number of Aisles and Most Popular Aisle
```{r}
aisle_info = instacart %>% 
  group_by(aisle_id, aisle) %>%
  summarize(n=n()) %>%
  arrange(desc(n))
```
The aisle in which the most items were ordered from is aisle 83 (fresh vegetables), with 150609 orders. The aisle in which the second most items were ordered from is aisle 24 (fresh fruits), with 150473 orders. The aisle in which the third most items were ordered from is aisle 123 (packaged vegetables fruits), with 78493 orders.

## Instacart Plot
```{r}
instacart_plot = filter(aisle_info, n >10000)
ggplot(
  instacart_plot, 
  aes(x = reorder(aisle, -n), y =n)) + 
  geom_point(color = 'blue') +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        axis.text = element_text(size = 7.5)) +
  ggtitle("Number of items ordered in each aisle \n(limited to aisles with more than 10,000 items ordered") +
labs(y = "# of Items Ordered",
     x = "Aisle")
```

## 3 Most popular items in each aisle

```{r}
library(knitr)

ranking = instacart %>%
filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>%
  group_by(aisle, product_name)%>%
  summarise(n = n()) %>%
  mutate(product_ranking = min_rank(desc(n))) %>% 
  filter(product_ranking < 4) %>%
  knitr::kable()
  ranking
```

## Mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week
```{r}
mean_hour_apples_icecream=instacart %>%
  filter(product_name == "Pink Lady Apples" | 
      product_name == "Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour= mean(order_hour_of_day)) %>%
  select(product_name, order_dow, mean_order_hour) %>%
  pivot_wider(
      names_from = order_dow,
      values_from = mean_order_hour
  ) %>%
  knitr::kable ()
mean_hour_apples_icecream
```


# Problem 2
```{r}
data("brfss_smart2010")
```

## Data cleaning
```{r}
brfss = brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health" |
           response == "Excellent" |
           response == "Very good" |
           response == "Good" |
           response == "Fair" |
           response == "Poor") %>%
mutate(response = factor(response, labels = c("Poor","Fair","Good","Very good", "Excellent" ))) %>%
select(-location_id, -data_value_footnote_symbol, -data_value_footnote)
brfss
```

## In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
brfss %>%
  filter(year == "2002") %>%
  group_by(locationabbr) %>%
  summarize(
    number_observations = n_distinct(geo_location)) %>%
    filter(number_observations > 6) %>%
  arrange(desc(number_observations))
```
PA, MA, NJ, CT, FL, and NC each were observed at 7 or more locations.


In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
brfss %>%
  filter(year == "2010") %>%
  group_by(locationabbr) %>%
  summarize(
    number_observations = n_distinct(geo_location)) %>%
    filter(number_observations > 6) %>%
  arrange(desc(number_observations))
```

FL, NJ, TX, CA, MD, NC, NE, WA, MA, NY, OH, CO, PA, and SC each were observed at 7 or more locations.

## Dataset limited to Excellent responses 
```{r}
excellent_data = brfss %>%
  filter(response == "Excellent") %>%
  group_by(year, locationabbr) %>%
  summarise(mean_data = mean(data_value, na.rm = TRUE)) %>%
  select(year, locationabbr, mean_data)
```

## Spaghetti Plot
```{r}
brfss_spaghetti =
  excellent_data %>%
  ggplot(aes(x = year, y = mean_data, group = locationabbr)) +
  geom_line(aes(color = locationabbr)) +
  labs(
    title = "Average value over time within 50 states",
    x = "Year",
    y = "Average Data Value"
  )
brfss_spaghetti
```

## Two-Panel Plot
```{r}
two_panel_data = brfss %>%
  filter(topic == "Overall Health" |
           response == "Excellent" |
           response == "Very good" |
           response == "Good" |
           response == "Fair" |
           response == "Poor",
           year == "2006" | 
           year== "2010",
           locationabbr == "NY")
ggplot(two_panel_data, aes(x = response, y = data_value, color = response)) +
  geom_boxplot() +
  facet_grid(. ~ year) +
labs(
    title = "Distribution of data values in NY \nfor 2006 & 2010 by response in NY",
    x = "Response",
    y = "Data Value"
  )

two_panel_data
```

# Problem 3

## Tidy Data
```{r}
prob3 = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(
    day = factor(day, labels = c("Monday", "Tuesday", "Wednesday",  "Thursday", "Friday", "Saturday", "Sunday")),
    dow = day == "Saturday" | day == "Sunday",
    dow = ifelse( dow == TRUE, "Weekend", "Weekday")
    ) %>%
  select(week, day, dow, everything())
```
There are `r nrow(prob3)` observations in this dataset (5 weeks-worth of accelerometer information for our subject). There are `r ncol(prob3)` in which we have a column for week number, day of the week, weekday vs. weekend, and 1440 columns indicating each minute's worth of activity associated with the particular day of interest. 


## Aggregate across minutes to create a total activity variable for each day
```{r}
agg_acc_data = prob3 %>%
  pivot_longer(activity_1:activity_1440,
               names_to = "activity_min",
               values_to = "activity") %>%
  group_by(day_id, day, week, dow) %>%
  summarize(daily_activity = sum(activity))
agg_acc_data

```
There do not appear to be any apparent trends. The daily activity appears to be quite high for most days. There are 2 Wednesdays that produced a daily activity value of 1440, which seems a bit odd.

## Single-panel plot that shows the 24-hour activity time courses for each day
```{r}
plot_agg_acc = agg_acc_data %>%
  ggplot(aes(x = day_id, y = daily_activity, color = day)) +
  geom_point() +
  geom_line () +
  labs(
    title = "Activity over course of day",
    x = "Day_id",
    y = "Total Activity Time"
  )
plot_agg_acc
```
This graph does not appear to have any apparent trends. It appears as though as time throughout the month went on, there was less activity on the subsequent Wednesdays and Thursdays after the first 10-15 days. At around day 24, there appears to be no/minimal activity for Wednesdays. Most days did not have an increase in activity over the course of the month; although Fridays had a slight increase in activity. Saturdays and Sundays appeared to be most consistent throughout the period of the month. There doesn't appear to be any consistencies when looking at this graph per consecutive days - this could be a good indication of a still-prevalent congestive heart failure problem in this individual.

